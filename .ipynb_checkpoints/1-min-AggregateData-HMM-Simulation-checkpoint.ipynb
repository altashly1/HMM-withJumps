{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2a9ef3-400a-4be7-874b-02f2fb425b5d",
   "metadata": {},
   "source": [
    "# Build a Hidden Markov Model Stock Excess Growth Rate for One Minute Aggregates\n",
    "This advanced example will build upon the previous example and will give students practice building an Observable Markov Model (OMM) of the excess growth rate of a ticker `SPY` using one-minute aggregate data (instead of daily data). The excess growth:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "R_{ij} = \\left(\\frac{1}{\\Delta{t}}\\right)\\cdot\\ln\\left(\\frac{S_{i,j}}{S_{i,j-1}}\\right) - \\bar{r}_{f}\n",
    "\\end{equation*}\n",
    "$$\n",
    "is a scaled return, assuming continuous compounding. The quantity $R_{ij}$ denotes the excess growth rate of equity $i$ at time $j$ (units: 1/year), $\\Delta{t}$ denotes the time-step between $j-1\\rightarrow{j}$ (units: years), $S_{i,\\star}$ denotes the share price of equity $i$ at time $\\star$, and $\\bar{r}_{f}$ denotes the annualized risk free rate.\n",
    "\n",
    "Describe the minute-to-minute variation of the excess growth using a fully observable Markov model $\\mathcal{M}$ represented by the tuple $\\mathcal{M} = (\\mathcal{S},\\mathcal{O},\\mathbf{P},\\mathbf{E})$; $\\mathcal{S}$ is the set of hidden states, $\\mathcal{O}$ is the set of observable states, $\\mathbf{T}$ is the transition matrix and $\\mathbf{E}$ is the emission matrix. Because we are fully observable, the emission matrix $\\mathbf{E} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\n",
    "\n",
    "## Learning objectives\n",
    "* __Task 1__: Construct the states $\\mathcal{S}$, the emission matrix $\\mathbf{E}$ and transition matrix $\\hat{\\mathbf{T}}$.\n",
    "    * Estimate the transition matrix $\\hat{\\mathbf{T}}$ from market data\n",
    "* __Task 2__: Simulate the Hidden Markov Model (HMM) for approximately one trading year in one-minute aggregates\n",
    "    * Generate the stationary distribution from the estimated $\\hat{\\mathbf{T}}$ matrix\n",
    "    * Implement the `MARKOV-SIMULATION` pseudo code from the lecture to generate hypothetical return sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92560d99-5a5b-430b-9749-4a56953d1e00",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We set up the computational environment by including the `Include.jl` file. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our lab problem.\n",
    "* For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/) and the [VLQuantitativeFinancePackage.jl documentation](https://github.com/varnerlab/VLQuantitativeFinancePackage.jl). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31591712-2a05-4ea5-92d4-9f04eff6dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea45c8-3766-461c-bb13-6b3c8b2b7ebb",
   "metadata": {},
   "source": [
    "## Prerequisites: Constants and process the one-minute aggregate share price data\n",
    "Before we set up the Markov model, we set some constant values that we'll use throughout the rest of the notebook. Then, we'll load and process the 1-minute aggregate share price data for `SPY.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df601a76-5fce-4b2c-9e5a-d36463ce606c",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac97a19b-610b-44cc-828b-46923ebaccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_free_rate = 0.0421; # 17-Jun-2024 10-year treasury\n",
    "Δt = (1/390)*(1/252); # 1-min aggregate trading periods in units of years\n",
    "ticker = \"SPY\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4296f95-d32d-42af-b263-cdd168f2db87",
   "metadata": {},
   "source": [
    "### Load and process `SPY` data\n",
    "The `SPY` data used in this project (the entire 2023 trading year) was downloaded from [oneminutedata.com](https://oneminutedata.com). The `SPY` dataset, which is stored as a [comma-separated value (CSV) file](https://en.wikipedia.org/wiki/Comma-separated_values), is loaded using the [read method exported by the CSV.jl package](https://github.com/JuliaData/CSV.jl) as a [DataFrame instance](https://github.com/JuliaData/DataFrames.jl). We save the `SPY` data in the `share_price_data_df::DataFrame` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "858b071e-e464-455c-8489-3a777a0896ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>96880×6 DataFrame</span></div><div style = \"float: right;\"><span style = \"font-style: italic;\">96855 rows omitted</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">date</th><th style = \"text-align: left;\">open</th><th style = \"text-align: left;\">high</th><th style = \"text-align: left;\">low</th><th style = \"text-align: left;\">close</th><th style = \"text-align: left;\">volume</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"String31\" style = \"text-align: left;\">String31</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Float64\" style = \"text-align: left;\">Float64</th><th title = \"Int64\" style = \"text-align: left;\">Int64</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">2023-01-03 09:30:00</td><td style = \"text-align: right;\">384.37</td><td style = \"text-align: right;\">384.46</td><td style = \"text-align: right;\">383.97</td><td style = \"text-align: right;\">384.13</td><td style = \"text-align: right;\">575276</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">2023-01-03 09:31:00</td><td style = \"text-align: right;\">384.13</td><td style = \"text-align: right;\">384.13</td><td style = \"text-align: right;\">383.59</td><td style = \"text-align: right;\">383.96</td><td style = \"text-align: right;\">293800</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">2023-01-03 09:32:00</td><td style = \"text-align: right;\">383.99</td><td style = \"text-align: right;\">384.42</td><td style = \"text-align: right;\">383.99</td><td style = \"text-align: right;\">384.154</td><td style = \"text-align: right;\">289329</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">2023-01-03 09:33:00</td><td style = \"text-align: right;\">384.16</td><td style = \"text-align: right;\">385.08</td><td style = \"text-align: right;\">384.15</td><td style = \"text-align: right;\">385.07</td><td style = \"text-align: right;\">300648</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">2023-01-03 09:34:00</td><td style = \"text-align: right;\">385.06</td><td style = \"text-align: right;\">385.12</td><td style = \"text-align: right;\">384.47</td><td style = \"text-align: right;\">384.51</td><td style = \"text-align: right;\">246156</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">2023-01-03 09:35:00</td><td style = \"text-align: right;\">384.54</td><td style = \"text-align: right;\">384.99</td><td style = \"text-align: right;\">384.53</td><td style = \"text-align: right;\">384.755</td><td style = \"text-align: right;\">230167</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">2023-01-03 09:36:00</td><td style = \"text-align: right;\">384.75</td><td style = \"text-align: right;\">384.78</td><td style = \"text-align: right;\">384.325</td><td style = \"text-align: right;\">384.53</td><td style = \"text-align: right;\">119297</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">2023-01-03 09:37:00</td><td style = \"text-align: right;\">384.54</td><td style = \"text-align: right;\">384.855</td><td style = \"text-align: right;\">384.375</td><td style = \"text-align: right;\">384.741</td><td style = \"text-align: right;\">129655</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">2023-01-03 09:38:00</td><td style = \"text-align: right;\">384.71</td><td style = \"text-align: right;\">384.89</td><td style = \"text-align: right;\">384.61</td><td style = \"text-align: right;\">384.79</td><td style = \"text-align: right;\">120879</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">2023-01-03 09:39:00</td><td style = \"text-align: right;\">384.8</td><td style = \"text-align: right;\">385.44</td><td style = \"text-align: right;\">384.77</td><td style = \"text-align: right;\">385.39</td><td style = \"text-align: right;\">210211</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">2023-01-03 09:40:00</td><td style = \"text-align: right;\">385.41</td><td style = \"text-align: right;\">385.784</td><td style = \"text-align: right;\">385.19</td><td style = \"text-align: right;\">385.748</td><td style = \"text-align: right;\">244204</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">2023-01-03 09:41:00</td><td style = \"text-align: right;\">385.73</td><td style = \"text-align: right;\">386.22</td><td style = \"text-align: right;\">385.73</td><td style = \"text-align: right;\">386.11</td><td style = \"text-align: right;\">340172</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">2023-01-03 09:42:00</td><td style = \"text-align: right;\">386.13</td><td style = \"text-align: right;\">386.43</td><td style = \"text-align: right;\">386.11</td><td style = \"text-align: right;\">386.18</td><td style = \"text-align: right;\">293164</td></tr><tr><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td><td style = \"text-align: right;\">&vellip;</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96869</td><td style = \"text-align: left;\">2023-12-29 15:48:00</td><td style = \"text-align: right;\">475.26</td><td style = \"text-align: right;\">475.3</td><td style = \"text-align: right;\">475.18</td><td style = \"text-align: right;\">475.275</td><td style = \"text-align: right;\">425067</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96870</td><td style = \"text-align: left;\">2023-12-29 15:49:00</td><td style = \"text-align: right;\">475.275</td><td style = \"text-align: right;\">475.305</td><td style = \"text-align: right;\">475.16</td><td style = \"text-align: right;\">475.21</td><td style = \"text-align: right;\">394915</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96871</td><td style = \"text-align: left;\">2023-12-29 15:50:00</td><td style = \"text-align: right;\">475.21</td><td style = \"text-align: right;\">475.29</td><td style = \"text-align: right;\">474.8</td><td style = \"text-align: right;\">475.24</td><td style = \"text-align: right;\">806848</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96872</td><td style = \"text-align: left;\">2023-12-29 15:51:00</td><td style = \"text-align: right;\">475.23</td><td style = \"text-align: right;\">475.575</td><td style = \"text-align: right;\">475.19</td><td style = \"text-align: right;\">475.56</td><td style = \"text-align: right;\">671094</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96873</td><td style = \"text-align: left;\">2023-12-29 15:52:00</td><td style = \"text-align: right;\">475.56</td><td style = \"text-align: right;\">475.63</td><td style = \"text-align: right;\">475.305</td><td style = \"text-align: right;\">475.34</td><td style = \"text-align: right;\">487935</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96874</td><td style = \"text-align: left;\">2023-12-29 15:53:00</td><td style = \"text-align: right;\">475.34</td><td style = \"text-align: right;\">475.435</td><td style = \"text-align: right;\">475.225</td><td style = \"text-align: right;\">475.24</td><td style = \"text-align: right;\">359230</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96875</td><td style = \"text-align: left;\">2023-12-29 15:54:00</td><td style = \"text-align: right;\">475.245</td><td style = \"text-align: right;\">475.42</td><td style = \"text-align: right;\">474.715</td><td style = \"text-align: right;\">474.75</td><td style = \"text-align: right;\">896854</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96876</td><td style = \"text-align: left;\">2023-12-29 15:55:00</td><td style = \"text-align: right;\">474.75</td><td style = \"text-align: right;\">474.935</td><td style = \"text-align: right;\">474.49</td><td style = \"text-align: right;\">474.919</td><td style = \"text-align: right;\">905376</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96877</td><td style = \"text-align: left;\">2023-12-29 15:56:00</td><td style = \"text-align: right;\">474.91</td><td style = \"text-align: right;\">475.28</td><td style = \"text-align: right;\">474.89</td><td style = \"text-align: right;\">475.12</td><td style = \"text-align: right;\">596916</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96878</td><td style = \"text-align: left;\">2023-12-29 15:57:00</td><td style = \"text-align: right;\">475.12</td><td style = \"text-align: right;\">475.16</td><td style = \"text-align: right;\">474.95</td><td style = \"text-align: right;\">474.98</td><td style = \"text-align: right;\">775939</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96879</td><td style = \"text-align: left;\">2023-12-29 15:58:00</td><td style = \"text-align: right;\">474.985</td><td style = \"text-align: right;\">475.2</td><td style = \"text-align: right;\">474.985</td><td style = \"text-align: right;\">475.1</td><td style = \"text-align: right;\">1267283</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">96880</td><td style = \"text-align: left;\">2023-12-29 15:59:00</td><td style = \"text-align: right;\">475.09</td><td style = \"text-align: right;\">475.78</td><td style = \"text-align: right;\">475.08</td><td style = \"text-align: right;\">475.4</td><td style = \"text-align: right;\">7370283</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& date & open & high & low & close & volume\\\\\n",
       "\t\\hline\n",
       "\t& String31 & Float64 & Float64 & Float64 & Float64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 2023-01-03 09:30:00 & 384.37 & 384.46 & 383.97 & 384.13 & 575276 \\\\\n",
       "\t2 & 2023-01-03 09:31:00 & 384.13 & 384.13 & 383.59 & 383.96 & 293800 \\\\\n",
       "\t3 & 2023-01-03 09:32:00 & 383.99 & 384.42 & 383.99 & 384.154 & 289329 \\\\\n",
       "\t4 & 2023-01-03 09:33:00 & 384.16 & 385.08 & 384.15 & 385.07 & 300648 \\\\\n",
       "\t5 & 2023-01-03 09:34:00 & 385.06 & 385.12 & 384.47 & 384.51 & 246156 \\\\\n",
       "\t6 & 2023-01-03 09:35:00 & 384.54 & 384.99 & 384.53 & 384.755 & 230167 \\\\\n",
       "\t7 & 2023-01-03 09:36:00 & 384.75 & 384.78 & 384.325 & 384.53 & 119297 \\\\\n",
       "\t8 & 2023-01-03 09:37:00 & 384.54 & 384.855 & 384.375 & 384.741 & 129655 \\\\\n",
       "\t9 & 2023-01-03 09:38:00 & 384.71 & 384.89 & 384.61 & 384.79 & 120879 \\\\\n",
       "\t10 & 2023-01-03 09:39:00 & 384.8 & 385.44 & 384.77 & 385.39 & 210211 \\\\\n",
       "\t11 & 2023-01-03 09:40:00 & 385.41 & 385.784 & 385.19 & 385.748 & 244204 \\\\\n",
       "\t12 & 2023-01-03 09:41:00 & 385.73 & 386.22 & 385.73 & 386.11 & 340172 \\\\\n",
       "\t13 & 2023-01-03 09:42:00 & 386.13 & 386.43 & 386.11 & 386.18 & 293164 \\\\\n",
       "\t14 & 2023-01-03 09:43:00 & 386.15 & 386.28 & 385.94 & 385.96 & 240719 \\\\\n",
       "\t15 & 2023-01-03 09:44:00 & 385.96 & 386.11 & 385.166 & 385.28 & 295728 \\\\\n",
       "\t16 & 2023-01-03 09:45:00 & 385.29 & 385.4 & 384.288 & 384.46 & 442074 \\\\\n",
       "\t17 & 2023-01-03 09:46:00 & 384.45 & 384.66 & 384.15 & 384.589 & 191741 \\\\\n",
       "\t18 & 2023-01-03 09:47:00 & 384.6 & 384.7 & 383.674 & 383.83 & 272632 \\\\\n",
       "\t19 & 2023-01-03 09:48:00 & 383.83 & 383.94 & 383.04 & 383.08 & 333801 \\\\\n",
       "\t20 & 2023-01-03 09:49:00 & 383.06 & 383.55 & 383.02 & 383.5 & 259997 \\\\\n",
       "\t21 & 2023-01-03 09:50:00 & 383.49 & 383.96 & 383.45 & 383.8 & 303930 \\\\\n",
       "\t22 & 2023-01-03 09:51:00 & 383.817 & 383.88 & 383.43 & 383.74 & 170317 \\\\\n",
       "\t23 & 2023-01-03 09:52:00 & 383.72 & 383.74 & 382.56 & 382.59 & 290852 \\\\\n",
       "\t24 & 2023-01-03 09:53:00 & 382.57 & 382.99 & 382.5 & 382.94 & 230649 \\\\\n",
       "\t25 & 2023-01-03 09:54:00 & 382.92 & 382.94 & 382.7 & 382.8 & 122413 \\\\\n",
       "\t26 & 2023-01-03 09:55:00 & 382.8 & 383.1 & 382.03 & 382.5 & 275856 \\\\\n",
       "\t27 & 2023-01-03 09:56:00 & 382.48 & 382.67 & 382.285 & 382.38 & 111089 \\\\\n",
       "\t28 & 2023-01-03 09:57:00 & 382.38 & 382.51 & 382.165 & 382.185 & 107867 \\\\\n",
       "\t29 & 2023-01-03 09:58:00 & 382.17 & 382.18 & 381.71 & 381.8 & 188672 \\\\\n",
       "\t30 & 2023-01-03 09:59:00 & 381.8 & 381.97 & 381.66 & 381.71 & 152104 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m96880×6 DataFrame\u001b[0m\n",
       "\u001b[1m   Row \u001b[0m│\u001b[1m date                \u001b[0m\u001b[1m open    \u001b[0m\u001b[1m high    \u001b[0m\u001b[1m low     \u001b[0m\u001b[1m close   \u001b[0m\u001b[1m volume  \u001b[0m\n",
       "       │\u001b[90m String31            \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Float64 \u001b[0m\u001b[90m Int64   \u001b[0m\n",
       "───────┼──────────────────────────────────────────────────────────────────\n",
       "     1 │ 2023-01-03 09:30:00  384.37   384.46   383.97   384.13    575276\n",
       "     2 │ 2023-01-03 09:31:00  384.13   384.13   383.59   383.96    293800\n",
       "     3 │ 2023-01-03 09:32:00  383.99   384.42   383.99   384.154   289329\n",
       "     4 │ 2023-01-03 09:33:00  384.16   385.08   384.15   385.07    300648\n",
       "     5 │ 2023-01-03 09:34:00  385.06   385.12   384.47   384.51    246156\n",
       "     6 │ 2023-01-03 09:35:00  384.54   384.99   384.53   384.755   230167\n",
       "     7 │ 2023-01-03 09:36:00  384.75   384.78   384.325  384.53    119297\n",
       "     8 │ 2023-01-03 09:37:00  384.54   384.855  384.375  384.741   129655\n",
       "     9 │ 2023-01-03 09:38:00  384.71   384.89   384.61   384.79    120879\n",
       "    10 │ 2023-01-03 09:39:00  384.8    385.44   384.77   385.39    210211\n",
       "    11 │ 2023-01-03 09:40:00  385.41   385.784  385.19   385.748   244204\n",
       "   ⋮   │          ⋮              ⋮        ⋮        ⋮        ⋮        ⋮\n",
       " 96871 │ 2023-12-29 15:50:00  475.21   475.29   474.8    475.24    806848\n",
       " 96872 │ 2023-12-29 15:51:00  475.23   475.575  475.19   475.56    671094\n",
       " 96873 │ 2023-12-29 15:52:00  475.56   475.63   475.305  475.34    487935\n",
       " 96874 │ 2023-12-29 15:53:00  475.34   475.435  475.225  475.24    359230\n",
       " 96875 │ 2023-12-29 15:54:00  475.245  475.42   474.715  474.75    896854\n",
       " 96876 │ 2023-12-29 15:55:00  474.75   474.935  474.49   474.919   905376\n",
       " 96877 │ 2023-12-29 15:56:00  474.91   475.28   474.89   475.12    596916\n",
       " 96878 │ 2023-12-29 15:57:00  475.12   475.16   474.95   474.98    775939\n",
       " 96879 │ 2023-12-29 15:58:00  474.985  475.2    474.985  475.1    1267283\n",
       " 96880 │ 2023-12-29 15:59:00  475.09   475.78   475.08   475.4    7370283\n",
       "\u001b[36m                                                        96859 rows omitted\u001b[0m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "share_price_data_df = CSV.read(joinpath(_PATH_TO_DATA, \"SPY-OHLC-1-min-aggregate-2023.csv\"), DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d5e18-4244-4555-9e45-4cade710f62d",
   "metadata": {},
   "source": [
    "Next, compute the [volume weighted average price](https://en.wikipedia.org/wiki/Volume-weighted_average_price#:~:text=In%20finance%2C%20volume%2Dweighted%20average,trading%20price%20for%20the%20period.) using the [vwap function](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/equity/#VLQuantitativeFinancePackage.vwap) for the `share_price_data_df::DataFrame.` We then compute the growth rate, i.e., time-scaled annualized return using [the `log_growth_matrix(...)` function](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/equity/#VLQuantitativeFinancePackage.log_growth_matrix), where $\\Delta{t}$ denotes the trading period, i.e., 1-minute in units of years. We save growth rate data in the `Rᵢ::Array{Float64,1}` array, where each row holds the growth rate for that trading time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e1d4cbe-0860-454d-b580-971910568a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96879-element Vector{Float64}:\n",
       " -25.370811956266607\n",
       "   6.433212218622656\n",
       "  34.47544744550597\n",
       "  16.70915720871667\n",
       "  13.550730179623784\n",
       "   2.661297195665073\n",
       "   4.2649203762947865\n",
       "   4.967265992331227\n",
       "  16.829867852257006\n",
       "  24.785890818294575\n",
       "  40.51341066698778\n",
       "  33.242673507082735\n",
       "  20.23424653772622\n",
       "   ⋮\n",
       "   0.3406377163070191\n",
       "   0.3162875911374961\n",
       "   0.6446669742260449\n",
       "   0.5396951870740953\n",
       "   0.3922352799593621\n",
       "   0.2880388669883392\n",
       "   0.7142093136693322\n",
       "   0.718297201272962\n",
       "   0.4765247924607921\n",
       "   0.6185536977465691\n",
       "   1.011400876667081\n",
       "   5.916606670551687"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Rᵢ = vwap(share_price_data_df)|> data -> log_growth_matrix(data, Δt = Δt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601615a8-1b27-49b3-934c-ace1caa02baf",
   "metadata": {},
   "source": [
    "## Task 1: Construct the states $\\mathcal{S}$, the emission matrix $\\mathbf{E}$ and transition matrix $\\hat{\\mathbf{T}}$\n",
    "First, consider the states $\\mathcal{S}$. Suppose we put a label (and number) the excess return values, ranging from `super bad = 1,` $\\dots$,` unchanged,` $\\dots$,` super good = N,` where if $R\\ll{0}$, then we are in the `super bad = 1,` state or $R\\gg{0}$ we are in the `super good = N` state (or we are someplace in between). \n",
    "* __Idea__: Use the [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) computed from the observed return series $R_{i,1}, \\dots, R_{i,n}$ to partition the actual (historical) excess growth rates into one of a fixed number of categories. Once we have the categories, compute the probability that category $i$ on the day $k$ is followed by category $j$ on the day $k+1$. These values are entries in the state transition matrix $\\hat{\\mathbf{T}}$.\n",
    "* To start, specify a value for the  `number_of_states` variable, where the `number_of_states` controls how many categories we are using when splitting up the excess return time series. We then set the `states` vector, which holds the states (numbered from `1`$\\rightarrow$`number_of_states`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08cd8fca-674f-4c1a-b066-2b0df00c68a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_states = 2000; # TODO: specify a value here\n",
    "states = range(1,stop=number_of_states) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968703b-ca20-4c32-b26d-db37ae503d0b",
   "metadata": {},
   "source": [
    "The `states` are hidden from the observer. Next, we set up the emissions matrix $\\mathbf{E}$. For this example, because the states are __fully observable__, i.e., we can see the states directly,  the emission matrix $\\mathbf{E}$ is the identity matrix $\\mathbf{I}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d04bba8a-a9b8-4697-b6dc-d06dd3643fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "E = diagm(ones(number_of_states));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad667f-3b10-4f70-8f90-86a2e2d727c0",
   "metadata": {},
   "source": [
    "### TODO: Estimate the transition matrix $\\hat{\\mathbf{T}}$ from market data\n",
    "To estimate the transition matrix $\\hat{\\mathbf{T}}$, we'll estimate the transition probabilities from the excess return data calculated in the `Prerequisites` section and saved in the `Rᵢ` variable:\n",
    "* Split the data into two blocks: the first (which we'll call the `in_sample_dataset`) will be used to estimate the elements of the matrix $\\hat{\\mathbf{T}}$, while the second (which we'll call `out_of_sample_dataset`) will be used for testing purposes (later). The fraction of data partitioned into the training dataset is controlled by the `split_fraction` variable.\n",
    "\n",
    "Set a value for the `split_fraction` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f660cb6-6e83-4f72-9da0-983c96b704ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.90; # TODO: specify a value here\n",
    "insample_end_index = round(split_fraction*length(Rᵢ), digits=0) |> Int\n",
    "in_sample_dataset = Rᵢ[1:insample_end_index]\n",
    "out_of_sample_dataset = Rᵢ[(insample_end_index+1):end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5b71b-ade2-47ec-8687-a8215d76c201",
   "metadata": {},
   "source": [
    "Next, we need to model the return data distribution to compute the [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function). The excess return distribution is a random variable that follows some [probability distribution function](https://en.wikipedia.org/wiki/Probability_distribution). We don't know what that distribution function is, but for now, we assume the excess returns follow a [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution)\n",
    "* We use the [fit_mle function](https://juliastats.org/Distributions.jl/stable/fit/#Distributions.fit_mle-Tuple{Any,%20Any}) exported by the [Distributions.jl package](https://github.com/JuliaStats/Distributions.jl) to fit a [Laplace distribution](https://en.wikipedia.org/wiki/Laplace_distribution) to our our data. We fit the distribution to the full dataset `Rₘ` and save the distribution in the `d` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66998588-5052-4465-93aa-b55bd0ceb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = fit_mle(InverseGaussian, Rᵢ); # use the *full* data set to establish the cutoff's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c04fe8-4be7-45f9-a2dd-6fed0dd9377d",
   "metadata": {},
   "source": [
    "Next, we generate the percentile cutoffs that we use to establish the bounds that correspond to each category of return, i.e., `super bad` or `super good`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3930c894-f2fa-429f-b457-11e520070faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_cutoff = range(0.0,stop=1.0,length=(number_of_states+1)) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f060727-c15a-41f0-a93b-c28c01ab7d00",
   "metadata": {},
   "source": [
    "Now that we have the cutoffs, compute the lower and upper bound for each potentiual category. To do this, we'll use the [quantile function](https://juliastats.org/Distributions.jl/stable/univariate/#Statistics.quantile-Tuple{UnivariateDistribution,%20Real}) exported by the [Distributions.jl package](https://github.com/JuliaStats/Distributions.jl). For a given `0 ≤ q ≤ 1`, `quantile(d, q)` is the smallest value `x` \n",
    "for which `cdf(d, x) ≥ q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3539c8bf-92cd-4b37-97c0-3985ce9085a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×3 Matrix{Float64}:\n",
       " 0.0         0.0147874     1.0\n",
       " 0.0147874   0.0161118     2.0\n",
       " 0.0161118   0.0170015     3.0\n",
       " 0.0170015   0.0176942     4.0\n",
       " 0.0176942   0.0182713     5.0\n",
       " 0.0182713   0.0187713     6.0\n",
       " 0.0187713   0.0192157     7.0\n",
       " 0.0192157   0.019618      8.0\n",
       " 0.019618    0.019987      9.0\n",
       " 0.019987    0.0203289    10.0\n",
       " 0.0203289   0.0206484    11.0\n",
       " 0.0206484   0.020949     12.0\n",
       " 0.020949    0.0212332    13.0\n",
       " ⋮                      \n",
       " 0.378739    0.384894   1989.0\n",
       " 0.384894    0.391652   1990.0\n",
       " 0.391652    0.39914    1991.0\n",
       " 0.39914     0.407533   1992.0\n",
       " 0.407533    0.417076   1993.0\n",
       " 0.417076    0.428128   1994.0\n",
       " 0.428128    0.441248   1995.0\n",
       " 0.441248    0.457375   1996.0\n",
       " 0.457375    0.478274   1997.0\n",
       " 0.478274    0.507927   1998.0\n",
       " 0.507927    0.559112   1999.0\n",
       " 0.559112   Inf         2000.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = Array{Float64,2}(undef, number_of_states, 3)\n",
    "for s ∈ states\n",
    "    bounds[s,1] = quantile(d,percentage_cutoff[s])\n",
    "    bounds[s,2] = quantile(d,percentage_cutoff[s+1])\n",
    "    bounds[s,3] = s\n",
    "end\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c00e10-e748-46a5-b691-6c6b76b9cae0",
   "metadata": {},
   "source": [
    "Now that we have the category bounds, let's take the excess return data and determine which state an excess return observation corresponds to. For each sample in the `in_sample_dataset`:\n",
    "* Classify the sample value into one of the possible categories. Let `state = 1` equal the worst return, and `state = number_of_states` equal the best return. Save these results in the `encoded_in_sample` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2d5bf65-1220-4d1d-85ae-7984043ca677",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_in_sample = Array{Int64,1}();\n",
    "for i ∈ eachindex(in_sample_dataset)\n",
    "    value = in_sample_dataset[i];\n",
    "\n",
    "    class_index = 1;\n",
    "    for s ∈ states\n",
    "        if (bounds[s,1] ≤ value && value < bounds[s,2])\n",
    "            class_index = s;\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "    push!(encoded_in_sample, class_index);\n",
    "end\n",
    "encoded_in_sample;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965ca7d-0448-4de1-ab47-107e35e66c7a",
   "metadata": {},
   "source": [
    "In the matrix $\\mathbf{T}$ compute the `counts` for transition from state `i` to state `j`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f2cf498-f07c-4b61-a0aa-14c108376738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×2000 Matrix{Float64}:\n",
       " 14833.0  142.0  86.0  58.0  46.0  42.0  …   0.0   0.0    0.0    0.0     2.0\n",
       "   142.0   32.0  25.0  15.0  11.0   9.0      0.0   0.0    0.0    0.0     0.0\n",
       "   103.0   21.0  18.0  11.0  12.0   7.0      0.0   0.0    0.0    0.0     0.0\n",
       "    65.0   15.0  13.0  14.0   6.0   6.0      0.0   0.0    0.0    0.0     0.0\n",
       "    59.0   11.0  14.0   3.0   8.0   7.0      0.0   0.0    0.0    0.0     0.0\n",
       "    37.0   14.0   6.0  10.0   4.0   5.0  …   0.0   0.0    0.0    0.0     0.0\n",
       "    44.0   12.0  11.0   4.0  11.0   6.0      0.0   0.0    0.0    0.0     0.0\n",
       "    33.0   14.0  12.0   5.0   5.0   6.0      0.0   0.0    0.0    0.0     0.0\n",
       "    27.0    6.0   4.0   3.0   7.0   0.0      0.0   0.0    0.0    0.0     0.0\n",
       "    15.0   10.0   6.0   6.0   5.0   4.0      0.0   0.0    0.0    0.0     0.0\n",
       "    28.0    8.0   9.0   0.0   0.0   4.0  …   0.0   0.0    0.0    0.0     1.0\n",
       "    13.0    5.0   7.0   2.0   1.0   2.0      0.0   0.0    0.0    0.0     0.0\n",
       "    16.0    5.0   0.0   3.0   3.0   1.0      0.0   0.0    0.0    0.0     0.0\n",
       "     ⋮                              ⋮    ⋱   ⋮                        \n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      3.0   2.0    6.0    5.0    24.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      2.0   2.0    2.0    3.0    15.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0  …   6.0   4.0    5.0    5.0    28.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      8.0   6.0    7.0    5.0    24.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      4.0   4.0    4.0   10.0    33.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      6.0   4.0    5.0    5.0    30.0\n",
       "     1.0    0.0   0.0   0.0   0.0   0.0      2.0   3.0    6.0   13.0    44.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0  …   3.0   9.0   15.0    9.0    47.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      5.0   9.0   11.0   18.0    83.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0      9.0  13.0   15.0   28.0    95.0\n",
       "     0.0    0.0   0.0   0.0   0.0   0.0     17.0  14.0   37.0   39.0   157.0\n",
       "     2.0    0.0   0.0   0.0   0.0   0.0     59.0  87.0  104.0  175.0  2102.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T = zeros(number_of_states, number_of_states)\n",
    "for i ∈ 2:insample_end_index\n",
    "    start_index = encoded_in_sample[i-1];\n",
    "    stop_index = encoded_in_sample[i];\n",
    "    T[start_index,stop_index] += 1;\n",
    "end\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b77c2-520c-48fa-98e6-4e2c3edc587a",
   "metadata": {},
   "source": [
    "From the `counts` matrix $\\mathbf{T}$, compute the transtion probability matrix $\\hat{\\mathbf{T}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bff6e82d-4db3-453b-99c1-094e2d34ac10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×2000 Matrix{Float64}:\n",
       " 0.928804     0.00889167  0.0053851  …  0.0        0.0        0.000125235\n",
       " 0.276803     0.0623782   0.0487329     0.0        0.0        0.0\n",
       " 0.237327     0.0483871   0.0414747     0.0        0.0        0.0\n",
       " 0.223368     0.0515464   0.0446735     0.0        0.0        0.0\n",
       " 0.208481     0.0388693   0.04947       0.0        0.0        0.0\n",
       " 0.152263     0.0576132   0.0246914  …  0.0        0.0        0.0\n",
       " 0.169231     0.0461538   0.0423077     0.0        0.0        0.0\n",
       " 0.157143     0.0666667   0.0571429     0.0        0.0        0.0\n",
       " 0.135        0.03        0.02          0.0        0.0        0.0\n",
       " 0.0815217    0.0543478   0.0326087     0.0        0.0        0.0\n",
       " 0.148936     0.0425532   0.0478723  …  0.0        0.0        0.00531915\n",
       " 0.081761     0.0314465   0.0440252     0.0        0.0        0.0\n",
       " 0.112676     0.0352113   0.0           0.0        0.0        0.0\n",
       " ⋮                                   ⋱                        \n",
       " 0.0          0.0         0.0           0.0472441  0.0393701  0.188976\n",
       " 0.0          0.0         0.0           0.0210526  0.0315789  0.157895\n",
       " 0.0          0.0         0.0        …  0.0384615  0.0384615  0.215385\n",
       " 0.0          0.0         0.0           0.0472973  0.0337838  0.162162\n",
       " 0.0          0.0         0.0           0.0240964  0.060241   0.198795\n",
       " 0.0          0.0         0.0           0.0304878  0.0304878  0.182927\n",
       " 0.00632911   0.0         0.0           0.0379747  0.0822785  0.278481\n",
       " 0.0          0.0         0.0        …  0.0694444  0.0416667  0.217593\n",
       " 0.0          0.0         0.0           0.0413534  0.0676692  0.31203\n",
       " 0.0          0.0         0.0           0.0418994  0.0782123  0.265363\n",
       " 0.0          0.0         0.0           0.0770833  0.08125    0.327083\n",
       " 0.000633914  0.0         0.0           0.0329635  0.0554675  0.666244"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T̂ = zeros(number_of_states, number_of_states)\n",
    "for row ∈ states\n",
    "    Z = sum(T[row,:]);\n",
    "    for col ∈ states\n",
    "        T̂[row,col] = (1/Z)*T[row,col]\n",
    "    end\n",
    "end\n",
    "T̂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695557e-ce39-4690-a1dd-7e4baf3ed5c4",
   "metadata": {},
   "source": [
    "## Task 2: Simulate the Hidden Markov Model (HMM)\n",
    "To do the simulation, we first build a [`MyHiddenMarkovModel` instance](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/markov/#VLQuantitativeFinancePackage.MyHiddenMarkovModel), which holds the data for our Markov model. We use a [`build(...)` function](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/markov/#VLQuantitativeFinancePackage.build-Tuple{Type{MyHiddenMarkovModel},%20NamedTuple}), which takes information about the `states,` the estimated transition matrix $\\hat{\\mathbf{T}}$, and the emission matrix $\\mathbf{E}$ and returns a [`MyHiddenMarkovModel` instance](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/markov/#VLQuantitativeFinancePackage.MyHiddenMarkovModel), which we save in the `model` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cc20d87-5a26-4c41-b013-8596faca3802",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build(MyHiddenMarkovModel, (\n",
    "    states = states,\n",
    "    T = T̂, \n",
    "    E = E\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2eea49-8ead-4b9f-8901-fb495ea07744",
   "metadata": {},
   "source": [
    "### TODO: Generate the stationary distribution from the estimated $\\hat{\\mathbf{T}}$ matrix\n",
    "Generate the stationary distribution for the estimated transition matrix $\\hat{\\mathbf{T}}$ and use it to construct a [Categorical distribution](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Categorical) representing the stationary distrubution, save the [Categorical distribution](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Categorical) in the `π̄`-variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca16cd88-78e7-44ca-ac95-06d240610c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "power_value = 10000;\n",
    "π̄ = (T̂^power_value) |> tmp -> Categorical(tmp[1,:]); # TODO: compute the stationary distribution (approx value is ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016132a6-4801-4242-b142-88bc97c4e10c",
   "metadata": {},
   "source": [
    "#### Check: Are you really at the stationary distribution?\n",
    "To compute the stationary distribution $\\bar{\\pi}$, we raise the transition matrix $\\hat{\\mathbf{T}}$ to the power of `power_value.` However, we don't know the value of `power_value` _a priori_. \n",
    "* We can test different values for `power_value` using the fact that stationary distribution is a rank one matrix. Thus, we can estimate the minimum value of `power_value` such that `T̂^power_value` has `rank = 1` and check this condition [using the Julia @assert macro](https://docs.julialang.org/en/v1/base/base/#Base.@assert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfdfd297-c5b7-40d8-b393-607d268fcef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@assert ((T̂^power_value) |> rank ) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49000384-30bd-41e6-b4c6-66cc41c1dcaf",
   "metadata": {},
   "source": [
    "### TODO: Implement the `MARKOV-SIMULATION` pseudo code to generate hypothetical return sequences\n",
    "*Generate 'number_of_paths' example sequences, each containing 'number_of_steps' days. These variables determine the length and number of our hypothetical return sequences. Assume each path starts from a draw from the stationary distribution `π̄.`\n",
    "* Save the simulated return sequences in the `archive::Array{Int64,2}(undef, number_of_steps, number_of_paths)` array, where the `row` index corresponds to a path, and the `col` index corresponds to a day.\n",
    "* We have implemented some shortcut logic to speed up the implementation. To evaluate the Markov model for a `number_of_steps,` issue the command `model(start_state, number_of_steps).` This will compute a chain with `number_of_steps` entries starting as `start_state` and return the simulated sequence as an `array.` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbb8b2db-0c36-46d7-b973-e8c43428cfa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87191×100 Matrix{Int64}:\n",
       "    1     1  1071     1  242   797  …  1328   797   612  1752   376   587\n",
       "    1     1   878     1   40   718     1706  1296    72  1905   244   157\n",
       "    1     1   963     1  113   649     1135   604    99  1219   964   391\n",
       "    1     1  1188     1   58  1368     1527   485    83  1152  1648  1541\n",
       "    1     1  1020     1   94  1616      727   381   252  1219  1903  1861\n",
       "    4     2  1393     1   76  1713  …   920    38    48  1742  1493  1935\n",
       "  141    89  1890     1   26  1363      486    36    73  1977  1038  1973\n",
       "  658   510  1905     1  133  1018     1157    85   155  1826  1632  1948\n",
       "  563    40  1254     1  184  1168     1012  1108   773  1917  1731  1521\n",
       "  286   240  1336     1  177   821      477  1730   580  1946  1581  1668\n",
       " 1088   291   306     1  224  1455  …   401  1107   961  1757   966  1879\n",
       " 1736   369    17     1    7  1305      140  1301    39  1634   373  1865\n",
       " 1902   812   110     1  163  1528      198  1644   512  1372   316  1709\n",
       "    ⋮                            ⋮  ⋱           ⋮                    \n",
       " 1358   643  1083   441    1     7       65     2   199     1   684  2000\n",
       " 1598   604   559  1126    1   176  …   585     5   119     1   765  2000\n",
       " 1628  1550   101  1653    1   627      545     1    22     1  1147  2000\n",
       " 1308   565     6   549    1  1188      132     1   193     1   442  2000\n",
       "  746   635    16   185    1  1271      126     1   152     1  1209  2000\n",
       "  920  1183    27    72    1   770      165     1   249     1  1151  1956\n",
       "    7   387    41    32    1   825  …   105     1    56     1  1141  1979\n",
       "   72  1129   118    44    1  1859       46     1   714     1   434  1981\n",
       "  110   851    80    21    1  1699      196     1  1141     1   744  1999\n",
       "  547  1106   537    29    1  1986     1404     1  1971     1   925  2000\n",
       "  349   824   450    35    1  1993     1392     1  1999     1   635  2000\n",
       "  367  1125   426     1    1  2000  …  1681     1  1993     1   897  2000"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_paths = 100;\n",
    "number_of_steps = insample_end_index; # average number of trading days per year\n",
    "encoded_archive = Array{Int64,2}(undef, number_of_steps, number_of_paths);\n",
    "for i ∈ 1:number_of_paths\n",
    "    start_state = rand(π̄);\n",
    "    tmp = model(start_state, number_of_steps)\n",
    "    for j ∈ 1:number_of_steps\n",
    "        encoded_archive[j,i] = tmp[j]\n",
    "    end\n",
    "end\n",
    "encoded_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff261b1d-798e-4e21-a0b2-f5e931a4e044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000×3 Matrix{Float64}:\n",
       " -25.3708      0.0147874     1.0\n",
       "   0.0147874   0.0161118     2.0\n",
       "   0.0161118   0.0170015     3.0\n",
       "   0.0170015   0.0176942     4.0\n",
       "   0.0176942   0.0182713     5.0\n",
       "   0.0182713   0.0187713     6.0\n",
       "   0.0187713   0.0192157     7.0\n",
       "   0.0192157   0.019618      8.0\n",
       "   0.019618    0.019987      9.0\n",
       "   0.019987    0.0203289    10.0\n",
       "   0.0203289   0.0206484    11.0\n",
       "   0.0206484   0.020949     12.0\n",
       "   0.020949    0.0212332    13.0\n",
       "   ⋮                      \n",
       "   0.378739    0.384894   1989.0\n",
       "   0.384894    0.391652   1990.0\n",
       "   0.391652    0.39914    1991.0\n",
       "   0.39914     0.407533   1992.0\n",
       "   0.407533    0.417076   1993.0\n",
       "   0.417076    0.428128   1994.0\n",
       "   0.428128    0.441248   1995.0\n",
       "   0.441248    0.457375   1996.0\n",
       "   0.457375    0.478274   1997.0\n",
       "   0.478274    0.507927   1998.0\n",
       "   0.507927    0.559112   1999.0\n",
       "   0.559112   40.5134     2000.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_sample_bounds = copy(bounds);\n",
    "actual_sample_bounds[1,1] = minimum(in_sample_dataset);\n",
    "actual_sample_bounds[end,2] = maximum(in_sample_dataset)\n",
    "actual_sample_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee888229-ce9d-4547-80b2-d952cd0f02b8",
   "metadata": {},
   "source": [
    "### Decode operation\n",
    "To turn the state $s\\in\\mathcal{S}$ back into an excess return value, we need to __decode__ the state. To do this, let's construct a [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) describing the observed return values associated with each state in the `encoded_in_sample` dataset.\n",
    "* We collect the observed excess return samples associated with a particular state $s$, store them in a `tmp` array, and use [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) to estimate a [Normal distribution exported from the Distributions.jl package](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79ed0a2e-d912-4c0b-89e2-272fe2019baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_distribution_model = Dict{Int,Normal}()\n",
    "for s ∈ states\n",
    "\n",
    "    # what indexes correspond to state s\n",
    "    index_collection_state_s = findall(x-> x == s, encoded_in_sample);\n",
    "    tmp = Array{Float64,1}();\n",
    "    for i ∈ index_collection_state_s\n",
    "        decoded_value = Rᵢ[i];\n",
    "        push!(tmp, decoded_value);\n",
    "    end\n",
    "    decode_distribution_model[s] = fit_mle(Normal,tmp);\n",
    "end\n",
    "decode_distribution_model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54ab30-26f6-48ce-82f9-774a74389bc0",
   "metadata": {},
   "source": [
    "Then, generate a random value for the excess return by sampling the appropriate [Normal distribution](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Normal). We develop `number_of_paths` trajectories, each containing `number_of_steps` values. We store these values in the `decoded_archive` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8637b805-d114-40d9-8be9-e59c2adf9072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87191×100 Matrix{Float64}:\n",
       " -0.965001    1.53357    0.0897358  …   0.173858   0.0496667   0.0608759\n",
       "  0.108595    0.957741   0.0771891      0.236487   0.0422551   0.0367049\n",
       "  0.843499   -0.798026   0.082507       0.100941   0.0825502   0.0504686\n",
       " -0.506217   -0.0313737  0.0984252      0.0956638  0.151508    0.134673\n",
       " -0.458532    0.225968   0.0862371      0.100993   0.234964    0.21137\n",
       "  0.0169789   0.015718   0.116981   …   0.171387   0.128356    0.260906\n",
       "  0.0355926   0.0314863  0.22672        0.330594   0.0874668   0.319636\n",
       "  0.0646796   0.056789   0.236306       0.196844   0.148768    0.276305\n",
       "  0.0595994   0.0261702  0.103892       0.245509   0.168632    0.132054\n",
       "  0.0447115   0.0420291  0.11129        0.273774   0.140496    0.1553\n",
       "  0.0909499   0.0449564  0.0458407  …   0.175211   0.0826818   0.220307\n",
       "  0.169898    0.0492975  0.0221524      0.149072   0.0495035   0.213366\n",
       "  0.23418     0.0733133  0.0332721      0.114837   0.046395    0.16384\n",
       "  ⋮                                 ⋱                         \n",
       "  0.113455    0.0638578  0.0905716     -0.325328   0.0660869   0.356431\n",
       "  0.143023    0.0617584  0.0593696  …  -1.52138    0.0706062   1.82377\n",
       "  0.148009    0.135986   0.0325137     -0.9735     0.0952777   1.07074\n",
       "  0.108671    0.0596837  0.0185807      0.818168   0.0532116  -1.72216\n",
       "  0.0695511   0.0634493  0.0219681     -0.416926   0.100136    5.13604\n",
       "  0.0798185   0.0980991  0.0241363     -0.360461   0.095599    0.288392\n",
       "  0.0190049   0.0502567  0.0262935  …   0.927143   0.0948513   0.336703\n",
       "  0.0298782   0.0939289  0.0338665      0.625455   0.0527687   0.346175\n",
       "  0.033288    0.0755818  0.0306513     -0.0549876  0.0694098   0.532843\n",
       "  0.0587514   0.0922214  0.0582182     -0.91862    0.0801087   0.864541\n",
       "  0.0481803   0.0739871  0.0536334      0.484545   0.0634576   2.51626\n",
       "  0.0492221   0.0936747  0.0523509  …   0.0647369  0.0783753   0.660101"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_sample_decoded_archive = Array{Float64,2}(undef, number_of_steps, number_of_paths);\n",
    "for i ∈ 1:number_of_paths\n",
    "    for j ∈ 1:number_of_steps\n",
    "        s = encoded_archive[j,i];\n",
    "        in_sample_decoded_archive[j,i] =  decode_distribution_model[s] |> d -> rand(d)\n",
    "    end\n",
    "end\n",
    "in_sample_decoded_archive # actual excess growth value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd08fd6-1141-4b44-a199-c8a6a843f5eb",
   "metadata": {},
   "source": [
    "### Visualize simulated excess growth rate distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ed9e13a-463c-436b-94bb-0dc846839229",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let\n",
    "    q = plot(); \n",
    "    density!(in_sample_decoded_archive[:,1], lw=2, c=:deepskyblue1, label=\"Simulated\", \n",
    "        bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent, \n",
    "        xlims=(bounds[1,2], bounds[end,1]))\n",
    "    for i ∈ 2:number_of_paths\n",
    "        density!(in_sample_decoded_archive[:,i], lw=1, c=:deepskyblue1, label=\"\")\n",
    "    end\n",
    "    density!(in_sample_dataset, c=:red, lw=3, label=\"Observed\")\n",
    "    xlabel!(\"Excess Annual Growth Rate $(ticker) (1/year)\", fontsize=18)\n",
    "    ylabel!(\"Probability Density (AU)\", fontsize=18)\n",
    "    current()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec234e-0b0c-47bd-b1a3-fc5cc506dccb",
   "metadata": {},
   "source": [
    "### Check: Are the predicted and observed return distributions the same? \n",
    "If our Markov model is correct, then the observed excess growth rate distribution and the excess growth distribution calculated by our model should look like they are drawn from the same distribution. To check this hypothesis, use [the ApproximateTwoSampleKSTest exported by the HypothesisTests.jl package](https://github.com/JuliaStats/HypothesisTests.jl) \n",
    "* `H0:null hypothesis` is that `x` and `y` are drawn from the same distribution against the `H1:alternative hypothesis` that `x` and `y` come from different distributions.\n",
    "\n",
    "We have `number_of_paths` example trajectories, so let's do the test on each sample and compute an overall expected score. Specify a `pvalue_cutoff` value to check against. If the test returns `pvalue > pvalue_cutoff,` then we fail to reject `H0:null hypothesis`, i.e., `x` and `y` appear to be drawn from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f8f968c-b823-4f24-b618-2f59ba866749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass percentage: 0.0%\n"
     ]
    }
   ],
   "source": [
    "pvalue_cutoff = 0.01; # 99% cutoff\n",
    "pass_counter = 0;\n",
    "for i ∈ 1:number_of_paths\n",
    "    test_value = ApproximateTwoSampleKSTest(in_sample_dataset,in_sample_decoded_archive[:,i]) |> pvalue\n",
    "    if (test_value > pvalue_cutoff)\n",
    "        pass_counter += 1 # we pass (fail to reject) x and y are from the same distribution\n",
    "    end\n",
    "end\n",
    "println(\"Pass percentage: $((pass_counter/number_of_paths)*100)%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bf16cd-800e-40c4-85f6-11b55517a040",
   "metadata": {},
   "source": [
    "## Task 3: Save the HMM model and other data to a file\n",
    "In the project and advanced example for this module, we'll use the hidden Markov Model (HMM) we developed here. Let's save the model to disk to save some time later. \n",
    "* Use the [save(...) method exported by the JLD2.jl package](https://github.com/JuliaIO/JLD2.jl.git) to write the model in a binary format. First, we specify a `path` in the `path_to_save_file` variable: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2286545a-4722-41af-8f5b-5f85ec660bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_save_file = joinpath(_PATH_TO_DATA,\"HMM-$(ticker)-1-min-aggregate.jld2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f2a16-49c3-419f-8fb6-8b8af7870f34",
   "metadata": {},
   "source": [
    "Then we write an `HDF5 binary file` holding our data to the location specified by `path_to_save_file.` We use the [save(...) function exported by the JLD2.jl package to write a binary save file](https://github.com/JuliaIO/JLD2.jl.git) (later we'll use to the `load(...)` function to reload this data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d8f149e8-6166-4b68-95ea-3743dbc13f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(path_to_save_file, Dict(\"model\"=>model, \n",
    "#         \"decode\"=>decode_distribution_model, \"stationary\"=>π̄, \n",
    "#         \"insampledataset\"=>in_sample_dataset, \"outofsampledataset\"=>out_of_sample_dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17389a-3c86-45fb-a649-d6611f6720ef",
   "metadata": {},
   "source": [
    "## Disclaimer and Risks\n",
    "__This content is offered solely for training and informational purposes__. No offer or solicitation to buy or sell securities or derivative products or any investment or trading advice or strategy is made, given, or endorsed by the teaching team. \n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance.  Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. You should decide solely based on your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.6",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a082ac95-d4c7-492d-bc3b-50fbb7caaaf9",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "This notebook explores the modeling of daily stock excess growth rates using Hidden Markov Models (HMMs). The analysis proceeds in two main stages:\n",
    "1.  A standard HMM is built from historical in-sample data.\n",
    "2.  This model is then tested against well-known \"stylized facts\" of financial returns, revealing its strengths (capturing return distributions) and weaknesses (failing to capture volatility clustering).\n",
    "\n",
    "To address these weaknesses, a modified HMM that incorporates price jumps is introduced and analyzed in `HMM-WithJumps-Simulation-Notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a9ef3-400a-4be7-874b-02f2fb425b5d",
   "metadata": {
    "editable": true,
    "id": "3c2a9ef3-400a-4be7-874b-02f2fb425b5d",
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " # Build a Hidden Markov Model of Daily Stock Excess Growth Rate\n",
    "We are constructing an Observable Markov Model (OMM) of the excess growth rate of a ticker `XYZ` where we define the excess growth as:\n",
    "$$\n",
    "\\begin{equation*}\n",
    "R_{ij} \\equiv \\left(\\frac{1}{\\Delta{t}}\\right)\\cdot\\ln\\left(\\frac{S_{i,j}}{S_{i,j-1}}\\right) - \\bar{r}_{f}\n",
    "\\end{equation*}\n",
    "$$\n",
    "where $R_{ij}$ denotes the excess growth rate of equity $i$ at time $j$, $\\Delta{t}$ denotes the time-step between $j-1\\rightarrow{j}$ (units: years), $S_{i,\\star}$ denotes the share price of equity $i$ at time $\\star$, and $\\bar{r}_{f}$ denotes the annualized risk free rate. In this work, we build a model of daily return.\n",
    "\n",
    "## Model\n",
    "Describe the day-to-day variation of the excess growth using a fully observable Markov model $\\mathcal{M}$ represented by the tuple $\\mathcal{M} = (\\mathcal{S},\\mathcal{O},\\mathbf{P},\\mathbf{E})$; $\\mathcal{S}$ is the set of hidden states, $\\mathcal{O}$ is the set of observable states, $\\mathbf{T}$ is the transition matrix, i.e., $t_{ij}\\in\\mathbf{T}$ is the probability of moving from hidden state $i$ to hidden state $j$ in the next time step, and $\\mathbf{E}$ is the emission matrix. Because we are fully observable, the emission matrix $\\mathbf{E} = \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix.\n",
    "\n",
    "## Objectives\n",
    "- **Task 1:** Characterize the continuous distribution of returns using MLE and MCMC, then discretize this distribution to build a Hidden Markov Model (HMM).\n",
    "- **Task 2:** Simulate the standard HMM and analyze its ability to reproduce key stylized facts of financial returns, particularly volatility clustering.\n",
    "- **Task 3:** Save the final model artifacts for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f687d9d-00ea-4858-82ba-644a73ecb996",
   "metadata": {
    "editable": true,
    "id": "1f687d9d-00ea-4858-82ba-644a73ecb996",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Setup\n",
    "We set up the computational environment by including the `Include.jl` file. The `Include.jl` file loads external packages, various functions that we will use in the exercise, and custom types to model the components of our lab problem.\n",
    "* For additional information on functions and types used in this material, see the [Julia programming language documentation](https://docs.julialang.org/en/v1/) and the [VLQuantitativeFinancePackage.jl documentation](https://github.com/varnerlab/VLQuantitativeFinancePackage.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5247f320-1d0d-48dd-8aa6-ac6e101ebbc3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "id": "31591712-2a05-4ea5-92d4-9f04eff6dd7c",
    "outputId": "5f95c1e8-e2f0-40ef-c301-8af48ed08dce",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "include(\"Include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae1b72-4b5d-4e15-bd97-b47b32af5563",
   "metadata": {},
   "source": [
    "## Setting Up Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d7320-62e2-44bb-911e-ef2e815be1ca",
   "metadata": {
    "id": "bc2d7320-62e2-44bb-911e-ef2e815be1ca"
   },
   "outputs": [],
   "source": [
    "risk_free_rate = 0.0421; # 17-Jun-2024 10-year treasury\n",
    "Δt = (1/252); # time step 1 x trading in units of years\n",
    "number_of_paths = 100; # number of potential futures should we look at\n",
    "blue_color = colorant\"rgb(68,152,242)\";\n",
    "ticker = \"AAPL\"; # This is the ticker we want to explore\n",
    "\n",
    "# palette -\n",
    "my_color_palette = Dict{Int64,RGB}();\n",
    "my_color_palette[0] = colorant\"#e5e5e5\";\n",
    "my_color_palette[1] = colorant\"#ff7d00\";\n",
    "my_color_palette[2] = colorant\"#14213d\";\n",
    "my_color_palette[3] = colorant\"#ffecd1\";\n",
    "my_color_palette[4] = colorant\"rgb(49,52,58)\";\n",
    "my_color_palette[5] = colorant\"#c0d6df\";\n",
    "my_color_palette[6] = colorant\"#000000\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eea45c8-3766-461c-bb13-6b3c8b2b7ebb",
   "metadata": {
    "id": "5eea45c8-3766-461c-bb13-6b3c8b2b7ebb"
   },
   "source": [
    "## Prerequisites: Load and clean the historical dataset\n",
    "We gathered a daily open-high-low-close `dataset` for each firm in the [S&P500](https://en.wikipedia.org/wiki/S%26P_500) from `01-03-2014` until `02-07-2025`, along with data for a few exchange-traded funds and volatility products during that time. In this block of code, we:\n",
    "* Load and clean the historical data; store the cleaned data in the `dataset` variable. We then calculate the expected excess return $\\mathbb{E}(R_{i})$ for each `ticker` in the `dataset.` Finally, you'll select a firm by changing the value in the `ticker` variable. We store the computed excess return for the `ticker` in the `Rᵢ` variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa22e1d-ba02-4d58-9f89-1192406a5127",
   "metadata": {
    "id": "8fa22e1d-ba02-4d58-9f89-1192406a5127"
   },
   "source": [
    "### Load the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b147bb-6a2c-4f3d-9ffd-9a2ff849045d",
   "metadata": {
    "id": "e4b147bb-6a2c-4f3d-9ffd-9a2ff849045d"
   },
   "source": [
    "The dataset for this analysis consists of daily open-high-low-close (OHLC) data for firms in the S&P 500, spanning from **January 3, 2014, to February 7, 2025**. This data was cleaned and split into training and testing sets in a separate preprocessing step.\n",
    "\n",
    "We begin by loading the pre-cleaned **training dataset** below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da692a0d-49a1-4b09-b8aa-61d9536133b9",
   "metadata": {
    "id": "da692a0d-49a1-4b09-b8aa-61d9536133b9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = MyPortfolioDataSet() |> x->x[\"train_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d9522d-9552-4d01-9973-6afc0fd36c35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset[\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960e868d-355a-4052-9997-eff5d0eb1752",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_number_trading_days = nrow(train_dataset[\"AAPL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e31e08-7373-49b1-930a-921958ee07e3",
   "metadata": {
    "id": "93e31e08-7373-49b1-930a-921958ee07e3"
   },
   "outputs": [],
   "source": [
    "dataset = Dict{String,DataFrame}();\n",
    "for (ticker,data) ∈ train_dataset\n",
    "    if (nrow(data) == maximum_number_trading_days)\n",
    "        dataset[ticker] = data;\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c9ab47-5fce-4422-81db-26fe609732ec",
   "metadata": {
    "id": "b2c9ab47-5fce-4422-81db-26fe609732ec"
   },
   "source": [
    "Lastly, let's get a sorted list of firms that we have in cleaned up `dataset` and save it in the `list_of_all_tickers::Array{String,1}` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6984d226-b228-4c6a-8a33-072a4e8cfaeb",
   "metadata": {
    "id": "6984d226-b228-4c6a-8a33-072a4e8cfaeb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_of_all_tickers = keys(dataset) |> collect |> x->sort(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da5236a-7c1d-4964-994f-baee817261c2",
   "metadata": {
    "id": "5da5236a-7c1d-4964-994f-baee817261c2"
   },
   "source": [
    "We compute the expected (annualized) log growth rate by passing the `dataset` and the entire list of firms we have in the dataset (held in the $N\\times{1}$ `list_of_all_tickers` array) to the [log_growth_matrix(...) method](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/equity/#VLQuantitativeFinancePackage.log_growth_matrix).\n",
    "* The result is stored in the `all_firms_return_matrix::Array{Float64,2}` variable, a $T-1\\times{N}$ array of log return values. Each row of `all_firms_return_matrix` corresponds to a time value, while each column corresponds to a firm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c897f26-4098-4138-8006-73775e5c9323",
   "metadata": {
    "id": "9c897f26-4098-4138-8006-73775e5c9323"
   },
   "outputs": [],
   "source": [
    "all_firms_excess_return_matrix = log_growth_matrix(dataset, list_of_all_tickers,\n",
    "    Δt = Δt, risk_free_rate = risk_free_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc486e89-301e-4559-b97b-4f1d8147a270",
   "metadata": {
    "id": "cc486e89-301e-4559-b97b-4f1d8147a270"
   },
   "source": [
    "Extract the growth rate for your `ticker::String` of interest, and save this in the `Rᵢ::Array{Float64,1}` array. This is the observed _in-sample_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df42777-fcae-4870-b284-4f0709a9a807",
   "metadata": {
    "id": "6df42777-fcae-4870-b284-4f0709a9a807"
   },
   "outputs": [],
   "source": [
    "Rᵢ = findfirst(x->x==ticker, list_of_all_tickers) |> i-> all_firms_excess_return_matrix[:,i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601615a8-1b27-49b3-934c-ace1caa02baf",
   "metadata": {
    "id": "601615a8-1b27-49b3-934c-ace1caa02baf"
   },
   "source": [
    "## Task 1: Characterizing and Discretizing the Return Distribution\n",
    "### Methodology: Building the Model from Data\n",
    "\n",
    "To construct the HMM, we first need to define its components from the historical returns data (`Rᵢ`):\n",
    "\n",
    "1.  **States:** The continuous range of daily returns is discretized into a finite number of states. Each state represents a specific market condition, from significant losses (State 1) to significant gains (State N). We define these states by calculating the quantiles of the historical return distribution.\n",
    "\n",
    "2.  **Transition Matrix:** The probability of moving from one state to another on the next day is estimated by observing the frequency of these transitions in the historical data.\n",
    "\n",
    "3.  **Emission Probabilities:** In this model, the emissions are deterministic. Being in a certain state means the return is within the predefined quantile range for that state.\n",
    "\n",
    "First, consider the states $\\mathcal{S}$. Suppose we number the excess return values, ranging from `super bad = 1,` $\\dots$,` unchanged,` $\\dots$,` super good = N,` where if $R\\ll{0}$, then we are in the `super bad = 1,` state or $R\\gg{0}$ we are in the `super good = N` state (or we are someplace in between).\n",
    "* __Idea__: Use the [cumulative distribution function](https://en.wikipedia.org/wiki/Cumulative_distribution_function) computed from the observed return series $R_{i,1}, \\dots, R_{i,n}$ to partition the actual (historical) excess returns into one of a fixed number of categories. Once we have the categories, compute the probability that category $i$ on the day $k$ is followed by category $j$ on the day $k+1$. These values are entries in the state transition matrix $\\hat{\\mathbf{T}}$.\n",
    "* To start, specify a value for the  `number_of_states` variable, where the `number_of_states` controls how many categories we are using when splitting up the excess return time series. We then set the `states` vector, which holds the states (numbered from `1`$\\rightarrow$`number_of_states`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cd8fca-674f-4c1a-b066-2b0df00c68a1",
   "metadata": {
    "id": "08cd8fca-674f-4c1a-b066-2b0df00c68a1"
   },
   "outputs": [],
   "source": [
    "number_of_states = 200; # specify a value here, too many states or too little states would impact the model performance. 100 should provide somewhat fair granuality \n",
    "states = range(1,stop=number_of_states) |> collect;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968703b-ca20-4c32-b26d-db37ae503d0b",
   "metadata": {
    "id": "f968703b-ca20-4c32-b26d-db37ae503d0b"
   },
   "source": [
    "The `states` are hidden from the observer. Next, we set up the emissions matrix $\\mathbf{E}$. For this example, because the states are __fully observable__, i.e., we can see the states directly,  the emission matrix $\\mathbf{E}$ is the identity matrix $\\mathbf{I}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04bba8a-a9b8-4697-b6dc-d06dd3643fd4",
   "metadata": {
    "id": "d04bba8a-a9b8-4697-b6dc-d06dd3643fd4"
   },
   "outputs": [],
   "source": [
    "E = diagm(ones(number_of_states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad667f-3b10-4f70-8f90-86a2e2d727c0",
   "metadata": {
    "id": "34ad667f-3b10-4f70-8f90-86a2e2d727c0"
   },
   "source": [
    "### Estimate the transition matrix $\\hat{\\mathbf{T}}$ from market data\n",
    "To estimate the transition matrix $\\hat{\\mathbf{T}}$, we'll estimate the transition probabilities from the excess return data calculated in the `Prerequisites` section and saved in the `Rᵢ` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f660cb6-6e83-4f72-9da0-983c96b704ef",
   "metadata": {
    "id": "9f660cb6-6e83-4f72-9da0-983c96b704ef"
   },
   "outputs": [],
   "source": [
    "in_sample_dataset = Rᵢ[1:(maximum_number_trading_days-1)] # set of excess return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5b71b-ade2-47ec-8687-a8215d76c201",
   "metadata": {
    "id": "ece5b71b-ade2-47ec-8687-a8215d76c201"
   },
   "source": [
    "### 1a. Analyzing the Underlying Continuous Distribution\n",
    "\n",
    "Before building our discrete-state HMM, we first seek to understand the underlying continuous probability distribution of the daily excess returns. A key part of modeling is choosing a distribution that accurately captures the stylized facts of the data, such as **\"fat tails\"** (a higher probability of extreme events than a normal distribution would suggest).\n",
    "\n",
    "In this section, we will:\n",
    "1.  Compare two candidate distributions: the **Laplace distribution** and the **Student's t-distribution**.\n",
    "2.  Compare two different methodologies for fitting these distributions: **Maximum Likelihood Estimation (MLE)** and the more robust **Markov Chain Monte Carlo (MCMC)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f4dd1a-468b-4465-ae8c-24b8a5ebcfea",
   "metadata": {},
   "source": [
    "First, we use our generic MCMC function to fit a **Student's t-distribution** to the return data. This is our primary candidate for a model that can capture fat tails. The output will be a `chain` object containing the full posterior distributions for the model's parameters: mean (`μ`), volatility (`σ`), and degrees-of-freedom (`ν`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399d2f3-0f9c-4e46-9871-1256d5181e5e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To learn the Student's t-distribution:\n",
    "chain_t = learn_distribution_mcmc(StudentTModel(), Rᵢ);\n",
    "\n",
    "# You can now analyze chain_t and chain_laplace as before\n",
    "plot(chain_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274f2e2-324e-482f-8807-bce348acf56e",
   "metadata": {},
   "source": [
    "Next, for a direct comparison, we fit the **Laplace distribution** using the same MCMC methodology. This will produce a `chain` object containing posteriors for the Laplace parameters: mean (`μ`) and scale (`b`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a23cf5-3e95-426c-87d8-75aabf7cf630",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To learn the Laplace distribution:\n",
    "chain_laplace = learn_distribution_mcmc(LaplaceModel(), Rᵢ);\n",
    "\n",
    "plot(chain_laplace)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510e0268-9961-4aab-b52b-54d142af5341",
   "metadata": {},
   "source": [
    "Finally, we will prepare all of our fitted models for a head-to-head comparison. We will use the traditional **Maximum Likelihood Estimation (MLE)** to fit a Laplace distribution as a baseline. Then, we will bring together all the models—both MCMC and MLE—for a final comparison using visual plots and quantitative statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6df405-2c1b-4683-8fbe-7fe70363cb1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Prepare All Four Models ---\n",
    "\n",
    "# 1. MLE Model\n",
    "Laplace_MLE = fit_mle(Laplace, Rᵢ);\n",
    "\n",
    "# 2. MCMC Models \n",
    "μ_t_mean = mean(chain_t[:μ])\n",
    "σ_t_mean = mean(chain_t[:σ])\n",
    "ν_t_mean = mean(chain_t[:ν])\n",
    "StudentT_MCMC = LocationScale(μ_t_mean, σ_t_mean, TDist(ν_t_mean))\n",
    "\n",
    "μ_laplace_mean = mean(chain_laplace[:μ])\n",
    "b_laplace_mean = mean(chain_laplace[:b])\n",
    "Laplace_MCMC = Laplace(μ_laplace_mean, b_laplace_mean);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed87718-0598-4377-afde-1aee0c3e9fdc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot 1: Laplace (MLE) vs. Data ---\n",
    "\n",
    "# Create a histogram of the actual returns data\n",
    "histogram(Rᵢ, normalize=:pdf, bins=200, label=\"Data Histogram\", c=:lightgray)\n",
    "\n",
    "# Plot the Laplace (MLE) distribution\n",
    "plot!(x -> pdf(Laplace_MLE, x), lw=3, label=\"Laplace (MLE)\", c=\"#0072B2\") # Blue\n",
    "\n",
    "title!(\"Laplace (MLE) Fit\")\n",
    "xlabel!(\"Excess Return\")\n",
    "ylabel!(\"Density\")\n",
    "xlims!(extrema(Rᵢ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e854ae2f-6b45-4aab-a8de-9736c209208b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot 3: Laplace (MCMC) vs. Data ---\n",
    "\n",
    "# Create a histogram of the actual returns data\n",
    "histogram(Rᵢ, normalize=:pdf, bins=200, label=\"Data Histogram\", c=:lightgray)\n",
    "\n",
    "# Plot the Laplace (MCMC) distribution\n",
    "plot!(x -> pdf(Laplace_MCMC, x), lw=3, label=\"Laplace (MCMC)\", c=\"#009E73\") # Bluish Green\n",
    "\n",
    "title!(\"Laplace (MCMC) Fit\")\n",
    "xlabel!(\"Excess Return\")\n",
    "ylabel!(\"Density\")\n",
    "xlims!(extrema(Rᵢ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c1e63-cebb-4862-88bd-2bda4fc3a76c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Plot 2: Student's t (MCMC) vs. Data ---\n",
    "\n",
    "# Create a histogram of the actual returns data\n",
    "histogram(Rᵢ, normalize=:pdf, bins=200, label=\"Data Histogram\", c=:lightgray)\n",
    "\n",
    "# Plot the Student's t (MCMC) distribution\n",
    "plot!(x -> pdf(StudentT_MCMC, x), lw=3, label=\"Student's t (MCMC)\", c=\"#D55E00\") # Vermillion/Orange\n",
    "\n",
    "title!(\"Student's t (MCMC) Fit\")\n",
    "xlabel!(\"Excess Return\")\n",
    "ylabel!(\"Density\")\n",
    "xlims!(extrema(Rᵢ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74669d6a-a4b0-48e9-b254-604ed90a5200",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ---  Visual Comparison of Four Distributions ---\n",
    "\n",
    "# Create a histogram of the actual returns data\n",
    "histogram(Rᵢ, normalize=:pdf, bins=200, label=\"Data Histogram\", c=:lightgray)\n",
    "\n",
    "# Plot the four models\n",
    "plot!(x -> pdf(Laplace_MLE, x), lw=3, label=\"Laplace (MLE)\")\n",
    "plot!(x -> pdf(StudentT_MCMC, x), lw=3, label=\"Student's t (MCMC)\", ls=:dash)\n",
    "plot!(x -> pdf(Laplace_MCMC, x), lw=3, label=\"Laplace (MCMC)\", ls=:dot)\n",
    "\n",
    "title!(\"Comparison of Fitted Distributions\")\n",
    "xlabel!(\"Excess Return\")\n",
    "ylabel!(\"Density\")\n",
    "xlims!(extrema(Rᵢ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3622c09-c7e9-4da3-931c-80b0f6ed1310",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- UPDATED: Quantitative Comparison of Four Distributions ---\n",
    "\n",
    "# A. Summary Statistics\n",
    "println(\"--- Summary Statistics ---\")\n",
    "println(\"Laplace (MLE):\")\n",
    "println(\"  Mean:      \", mean(Laplace_MLE))\n",
    "println(\"  Variance:  \", var(Laplace_MLE))\n",
    "println(\"  Kurtosis:  \", kurtosis(Laplace_MLE))\n",
    "\n",
    "println(\"\\nStudent's t (MCMC):\")\n",
    "println(\"  Mean:      \", mean(StudentT_MCMC))\n",
    "println(\"  Variance:  \", var(StudentT_MCMC))\n",
    "println(\"  Kurtosis:  \", kurtosis(StudentT_MCMC))\n",
    "\n",
    "println(\"\\nLaplace (MCMC):\")\n",
    "println(\"  Mean:      \", mean(Laplace_MCMC))\n",
    "println(\"  Variance:  \", var(Laplace_MCMC))\n",
    "println(\"  Kurtosis:  \", kurtosis(Laplace_MCMC))\n",
    "\n",
    "\n",
    "# B. AIC Comparison (Lower is Better)\n",
    "log_like_L_MLE = loglikelihood(Laplace_MLE, Rᵢ)\n",
    "log_like_T_MCMC = loglikelihood(StudentT_MCMC, Rᵢ)\n",
    "log_like_L_MCMC = loglikelihood(Laplace_MCMC, Rᵢ)\n",
    "\n",
    "aic_L_MLE = 2*2 - 2*log_like_L_MLE\n",
    "aic_T_MCMC = 2*3 - 2*log_like_T_MCMC\n",
    "aic_L_MCMC = 2*2 - 2*log_like_L_MCMC\n",
    "\n",
    "println(\"\\n--- Model Comparison (Lower AIC is Better) ---\")\n",
    "println(\"AIC for Laplace (MLE):       \", aic_L_MLE)\n",
    "println(\"AIC for Laplace (MCMC):      \", aic_L_MCMC)\n",
    "println(\"AIC for Student's t (MCMC):  \", aic_T_MCMC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2720798a-a35a-4149-8c25-32f83d7beeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of these to be your distribution in the variable d that will be used for generating CDF\n",
    "d = Laplace_MLE;  # StudentT_MCMC or Laplace_MCMC or Laplace_MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b417ea-db9b-4110-b837-164543bc7aa5",
   "metadata": {
    "id": "60b417ea-db9b-4110-b837-164543bc7aa5",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "\n",
    "    # regions of return -\n",
    "    number_of_samples = 1000;\n",
    "    minimum_obs_growth = minimum(in_sample_dataset);\n",
    "    maximum_obs_growth = maximum(in_sample_dataset);\n",
    "    RA = range(minimum_obs_growth,stop = maximum_obs_growth, length = number_of_samples) |> collect;\n",
    "\n",
    "    X = Array{Float64,2}(undef, number_of_samples,2);\n",
    "    for i ∈ eachindex(RA)\n",
    "\n",
    "        X[i,1] = RA[i];\n",
    "        X[i,2] = cdf(d, RA[i]);\n",
    "    end\n",
    "\n",
    "    plot(X[:,1], X[:,2], lw=3, c=:navy, label=\"Observed P(Rᵢ ≤ x)\", xminorticks=5, yminorticks=5)\n",
    "    plot!(X[:,1], 1 .- X[:,2], lw=3, c=:deepskyblue1, label=\"Observed P(Rᵢ > x)\", legend=:left)\n",
    "\n",
    "    xlabel!(\"$(ticker) Excess Annual Growth Rate (1/year)\", fontsize=18);\n",
    "    ylabel!(\"Cumulative Probability\", fontsize=18);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c04fe8-4be7-45f9-a2dd-6fed0dd9377d",
   "metadata": {
    "id": "07c04fe8-4be7-45f9-a2dd-6fed0dd9377d"
   },
   "source": [
    "Next, we generate the percentile cutoffs that we use to establish the bounds that correspond to each category of return, i.e., `super bad` or `super good`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3930c894-f2fa-429f-b457-11e520070faf",
   "metadata": {
    "id": "3930c894-f2fa-429f-b457-11e520070faf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "percentage_cutoff = range(0.0,stop=1.0,length=(number_of_states+1)) |> collect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f060727-c15a-41f0-a93b-c28c01ab7d00",
   "metadata": {
    "id": "5f060727-c15a-41f0-a93b-c28c01ab7d00"
   },
   "source": [
    "Now that we have the cutoffs, compute the lower and upper bound for each potentiual category. To do this, we'll use the [quantile function](https://juliastats.org/Distributions.jl/stable/univariate/#Statistics.quantile-Tuple{UnivariateDistribution,%20Real}) exported by the [Distributions.jl package](https://github.com/JuliaStats/Distributions.jl). For a given `0 ≤ q ≤ 1`, `quantile(d, q)` is the smallest value `x`\n",
    "for which `cdf(d, x) ≥ q`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3539c8bf-92cd-4b37-97c0-3985ce9085a3",
   "metadata": {
    "id": "3539c8bf-92cd-4b37-97c0-3985ce9085a3"
   },
   "outputs": [],
   "source": [
    "bounds = Array{Float64,2}(undef, number_of_states, 3)\n",
    "for s ∈ states\n",
    "    bounds[s,1] = quantile(d,percentage_cutoff[s])\n",
    "    bounds[s,2] = quantile(d,percentage_cutoff[s+1])\n",
    "    bounds[s,3] = s\n",
    "end\n",
    "bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c00e10-e748-46a5-b691-6c6b76b9cae0",
   "metadata": {
    "id": "66c00e10-e748-46a5-b691-6c6b76b9cae0"
   },
   "source": [
    "Now that we have the category bounds, let's take the excess return data and determine which state an excess return observation corresponds to. For each sample in the `in_sample_dataset`:\n",
    "* Classify the sample value into one of the possible categories. Let `state = 1` equal the worst return, and `state = number_of_states` equal the best return. Save these results in the `encoded_in_sample` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5bf65-1220-4d1d-85ae-7984043ca677",
   "metadata": {
    "id": "f2d5bf65-1220-4d1d-85ae-7984043ca677",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_in_sample = Array{Int64,1}();\n",
    "for i ∈ eachindex(in_sample_dataset)\n",
    "    value = in_sample_dataset[i];\n",
    "\n",
    "    class_index = 1;\n",
    "    for s ∈ states\n",
    "        if (bounds[s,1] ≤ value && value < bounds[s,2])\n",
    "            class_index = s;\n",
    "            break;\n",
    "        end\n",
    "    end\n",
    "    push!(encoded_in_sample, class_index);\n",
    "end\n",
    "encoded_in_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e2716c-6a0c-4722-a0dd-8dc428113005",
   "metadata": {},
   "source": [
    "This cell can be used to check the most frequent hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aa450c-a526-4eca-b74c-76d680d9e284",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = countmap(encoded_in_sample) |> argmax;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965ca7d-0448-4de1-ab47-107e35e66c7a",
   "metadata": {
    "id": "0965ca7d-0448-4de1-ab47-107e35e66c7a"
   },
   "source": [
    "In the matrix $\\mathbf{T}$ compute the `counts` for transition from state `i` to state `j`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2cf498-f07c-4b61-a0aa-14c108376738",
   "metadata": {
    "id": "7f2cf498-f07c-4b61-a0aa-14c108376738"
   },
   "outputs": [],
   "source": [
    "T = zeros(number_of_states, number_of_states)\n",
    "number_insample = length(encoded_in_sample);\n",
    "for i ∈ 2:number_insample\n",
    "    start_index = encoded_in_sample[i-1];\n",
    "    stop_index = encoded_in_sample[i];\n",
    "    T[start_index,stop_index] += 1;\n",
    "end\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67b77c2-520c-48fa-98e6-4e2c3edc587a",
   "metadata": {
    "id": "d67b77c2-520c-48fa-98e6-4e2c3edc587a"
   },
   "source": [
    "From the `counts` matrix $\\mathbf{T}$, compute the transtion probability matrix $\\hat{\\mathbf{T}}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6e82d-4db3-453b-99c1-094e2d34ac10",
   "metadata": {
    "id": "bff6e82d-4db3-453b-99c1-094e2d34ac10"
   },
   "outputs": [],
   "source": [
    "T̂ = zeros(number_of_states, number_of_states)\n",
    "for row ∈ states\n",
    "    Z = sum(T[row,:]);\n",
    "    for col ∈ states\n",
    "        T̂[row,col] = (1/Z)*T[row,col]\n",
    "    end\n",
    "end\n",
    "T̂"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2eea49-8ead-4b9f-8901-fb495ea07744",
   "metadata": {
    "id": "7a2eea49-8ead-4b9f-8901-fb495ea07744"
   },
   "source": [
    "### Generate the stationary distribution from the estimated $\\hat{\\mathbf{T}}$ matrix\n",
    "Generate the stationary distribution for the estimated transition matrix $\\hat{\\mathbf{T}}$ and use it to construct a [Categorical distribution](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Categorical) representing the stationary distrubution, save the [Categorical distribution](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Categorical) in the `π̄`-variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca16cd88-78e7-44ca-ac95-06d240610c44",
   "metadata": {
    "id": "ca16cd88-78e7-44ca-ac95-06d240610c44"
   },
   "outputs": [],
   "source": [
    "power_value = 50;\n",
    "π̄ = (T̂^power_value) |> tmp -> Categorical(tmp[1,:]); # compute the stationary distribution (approx value is ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695557e-ce39-4690-a1dd-7e4baf3ed5c4",
   "metadata": {
    "id": "a695557e-ce39-4690-a1dd-7e4baf3ed5c4"
   },
   "source": [
    "## Task 2: In-Sample Analysis of the Standard HMM\n",
    "To do the simulation, we first build a [`MyHiddenMarkovModel` instance](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/markov/#VLQuantitativeFinancePackage.MyHiddenMarkovModel), which holds the data for our Markov model. We use a [`build(...)` function](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/markov/#VLQuantitativeFinancePackage.build-Tuple{Type{MyHiddenMarkovModel},%20NamedTuple}), which takes information about the `states,` the estimated transition matrix $\\hat{\\mathbf{T}}$, and the emission matrix $\\mathbf{E}$ and returns a [`MyHiddenMarkovModel` instance](https://varnerlab.github.io/VLQuantitativeFinancePackage.jl/dev/markov/#VLQuantitativeFinancePackage.MyHiddenMarkovModel), which we save in the `model` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc20d87-5a26-4c41-b013-8596faca3802",
   "metadata": {
    "id": "7cc20d87-5a26-4c41-b013-8596faca3802"
   },
   "outputs": [],
   "source": [
    "model = build(MyHiddenMarkovModel, (\n",
    "    states = states,\n",
    "    T = T̂,\n",
    "    E = E\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49000384-30bd-41e6-b4c6-66cc41c1dcaf",
   "metadata": {
    "id": "49000384-30bd-41e6-b4c6-66cc41c1dcaf"
   },
   "source": [
    "### Implement the `MARKOV-SIMULATION` to generate hypothetical return sequences\n",
    "Generate 'number_of_paths' example sequences, each containing 'number_of_steps' days. These variables determine the length and number of our hypothetical return sequences. Assume each path starts from a draw from the stationary distribution `π̄.`\n",
    "* Save the simulated return sequences in the `archive::Array{Int64,2}(undef, number_of_steps, number_of_paths)` array, where the `row` index corresponds to a path, and the `col` index corresponds to a day.\n",
    "* We have implemented some shortcut logic to speed up the implementation. To evaluate the Markov model for a `number_of_steps,` issue the command `model(start_state, number_of_steps).` This will compute a chain with `number_of_steps` starting as `start_state` and return the simulated sequence as an `array.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8b2db-0c36-46d7-b973-e8c43428cfa4",
   "metadata": {
    "id": "dbb8b2db-0c36-46d7-b973-e8c43428cfa4"
   },
   "outputs": [],
   "source": [
    "number_of_steps = maximum_number_trading_days-1;\n",
    "encoded_archive = Array{Int64,2}(undef, number_of_steps, number_of_paths);\n",
    "for i ∈ 1:number_of_paths\n",
    "    start_state = rand(π̄);\n",
    "    tmp = model(start_state, number_of_steps)\n",
    "    for j ∈ 1:number_of_steps\n",
    "        encoded_archive[j,i] = tmp[j]\n",
    "    end\n",
    "end\n",
    "encoded_archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff261b1d-798e-4e21-a0b2-f5e931a4e044",
   "metadata": {
    "id": "ff261b1d-798e-4e21-a0b2-f5e931a4e044"
   },
   "outputs": [],
   "source": [
    "actual_sample_bounds = copy(bounds);\n",
    "actual_sample_bounds[1,1] = minimum(in_sample_dataset);\n",
    "actual_sample_bounds[end,2] = maximum(in_sample_dataset)\n",
    "actual_sample_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee888229-ce9d-4547-80b2-d952cd0f02b8",
   "metadata": {
    "id": "ee888229-ce9d-4547-80b2-d952cd0f02b8"
   },
   "source": [
    "### Decode operation\n",
    "To turn the state $s\\in\\mathcal{S}$ back into an excess return value, we need to __decode__ the state. To do this, let's construct a [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) describing the observed return values associated with each state in the `encoded_in_sample` dataset.\n",
    "* We collect the observed excess return samples associated with a particular state $s$, store them in a `tmp` array, and use [Maximum Likelihood Estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) to estimate a [Normal distribution exported from the Distributions.jl package](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed0a2e-d912-4c0b-89e2-272fe2019baa",
   "metadata": {
    "id": "79ed0a2e-d912-4c0b-89e2-272fe2019baa"
   },
   "outputs": [],
   "source": [
    "decode_distribution_model = Dict{Int,Normal}()\n",
    "for s ∈ states\n",
    "\n",
    "    # what indexes correspond to state s\n",
    "    index_collection_state_s = findall(x-> x == s, encoded_in_sample);\n",
    "    tmp = Array{Float64,1}();\n",
    "    for i ∈ index_collection_state_s\n",
    "        decoded_value = Rᵢ[i];\n",
    "        push!(tmp, decoded_value);\n",
    "    end\n",
    "    decode_distribution_model[s] = fit_mle(Normal,tmp);\n",
    "end\n",
    "decode_distribution_model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a54ab30-26f6-48ce-82f9-774a74389bc0",
   "metadata": {
    "id": "3a54ab30-26f6-48ce-82f9-774a74389bc0"
   },
   "source": [
    "Then, generate a random value for the excess return by sampling the appropriate [Normal distribution](https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Normal). We develop `number_of_paths` trajectories, each containing `number_of_steps` values. We store these values in the `decoded_archive` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8637b805-d114-40d9-8be9-e59c2adf9072",
   "metadata": {
    "id": "8637b805-d114-40d9-8be9-e59c2adf9072"
   },
   "outputs": [],
   "source": [
    "in_sample_decoded_archive = Array{Float64,2}(undef, number_of_steps, number_of_paths);\n",
    "for i ∈ 1:number_of_paths\n",
    "    for j ∈ 1:number_of_steps\n",
    "        s = encoded_archive[j,i];\n",
    "        in_sample_decoded_archive[j,i] =  decode_distribution_model[s] |> d -> rand(d)\n",
    "    end\n",
    "end\n",
    "in_sample_decoded_archive # actual excess growth value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd08fd6-1141-4b44-a199-c8a6a843f5eb",
   "metadata": {
    "id": "fdd08fd6-1141-4b44-a199-c8a6a843f5eb"
   },
   "source": [
    "### Visualize an example in-sample return trajectory\n",
    "`Unhide` the code block to see how we plotted the observed (red) and simulated (blue) excess growth rate values for a randomly selected sample model generated sample path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff8f397-6b0a-446b-91fb-f1a6b79c05da",
   "metadata": {
    "id": "0ff8f397-6b0a-446b-91fb-f1a6b79c05da"
   },
   "source": [
    "`Unhide` the code block below to see how we plotted the observed and simulated excess annual growth rate distribution for the in-sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89ebce-06a7-42ef-b4d6-6811b3faab75",
   "metadata": {
    "id": "8d89ebce-06a7-42ef-b4d6-6811b3faab75",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    # Select a random path to plot, just like in your working example\n",
    "    index_to_plot = rand(1:number_of_paths)\n",
    "\n",
    "    # Plot the SIMULATED data first, creating the plot\n",
    "    plot(in_sample_decoded_archive[:, index_to_plot], linetype=:steppost, label=\"Simulated (Path $(index_to_plot))\")\n",
    "\n",
    "    # Add the OBSERVED data to the plot, slicing it to match the simulation length\n",
    "    plot!(in_sample_dataset[1:(number_of_steps - 1)], linetype=:steppost, c=:red, label=\"Observed\")\n",
    "    \n",
    "    # Add labels and a title\n",
    "    xlabel!(\"Trading Day Index\")\n",
    "    ylabel!(\"Excess Return (1/yr)\")\n",
    "    title!(\"In-Sample Return Trajectories (Non-Jump Model)\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ec234e-0b0c-47bd-b1a3-fc5cc506dccb",
   "metadata": {
    "id": "29ec234e-0b0c-47bd-b1a3-fc5cc506dccb"
   },
   "source": [
    "### Check: Are the Simulated and observed in-sample distributions the same?\n",
    "If our Markov model is correct, then the observed excess growth rate distribution and the excess growth distribution calculated by our model should look like they are drawn from the same distribution. To check this hypothesis, use [the ApproximateTwoSampleKSTest exported by the HypothesisTests.jl package](https://github.com/JuliaStats/HypothesisTests.jl) with the hypotheses:\n",
    "* `H0:null hypothesis` is that `x` and `y` are drawn from the same distribution against the `H1:alternative hypothesis` that `x` and `y` come from different distributions.\n",
    "\n",
    "Let's run [the ApproximateTwoSampleKSTest function](https://github.com/JuliaStats/HypothesisTests.jl) on a single (randomly selected) example trajectory to see what happens (most of the time, we fail to reject `H0`, i.e., the test suggests `x` and `y` are from the same distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9e13a-463c-436b-94bb-0dc846839229",
   "metadata": {
    "id": "0ed9e13a-463c-436b-94bb-0dc846839229",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "let\n",
    "    q = plot();\n",
    "    density!(in_sample_decoded_archive[:,1], lw=2, c=:deepskyblue1, label=\"Simulated\",\n",
    "        bg=\"gray95\", background_color_outside=\"white\", framestyle = :box, fg_legend = :transparent)\n",
    "    for i ∈ 2:number_of_paths\n",
    "        density!(in_sample_decoded_archive[:,i], lw=1, c=:deepskyblue1, label=\"\")\n",
    "    end\n",
    "    density!(in_sample_dataset, c=:red, lw=3, label=\"Observed\")\n",
    "    xlabel!(\"Excess Annual Growth Rate $(ticker) (1/year)\", fontsize=18)\n",
    "    ylabel!(\"Probability Density (AU)\", fontsize=18)\n",
    "    current()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9586c74c-7ef9-4909-b2ae-93c9d59b0884",
   "metadata": {
    "id": "9586c74c-7ef9-4909-b2ae-93c9d59b0884"
   },
   "outputs": [],
   "source": [
    "ApproximateTwoSampleKSTest(in_sample_dataset,in_sample_decoded_archive[:,rand(1:number_of_paths)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3bab38-c2c9-4ea0-ba64-460496e67196",
   "metadata": {
    "id": "ed3bab38-c2c9-4ea0-ba64-460496e67196"
   },
   "source": [
    "However, we have `number_of_paths` example trajectories (not just one), so let's do the same test on each sample and compute an overall expected score. Specify a `pvalue_cutoff` value to check against. If the test returns `pvalue > pvalue_cutoff,` then we fail to reject `H0:null hypothesis`, i.e., `x` and `y` appear to be drawn from the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f968c-b823-4f24-b618-2f59ba866749",
   "metadata": {
    "id": "0f8f968c-b823-4f24-b618-2f59ba866749"
   },
   "outputs": [],
   "source": [
    "let\n",
    "    pvalue_cutoff = 0.05; # cutoff\n",
    "    pass_counter = 0;\n",
    "    for i ∈ 1:number_of_paths\n",
    "        test_value = ApproximateTwoSampleKSTest(in_sample_dataset,in_sample_decoded_archive[:,i]) |> pvalue\n",
    "        if (test_value > pvalue_cutoff)\n",
    "            pass_counter += 1 # we pass (fail to reject) x and y are from the same distribution\n",
    "        end\n",
    "    end\n",
    "    println(\"Pass percentage: $((pass_counter/number_of_paths)*100)%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0fa81-d123-4247-aa13-71a483ff0997",
   "metadata": {},
   "source": [
    "#### Anderson-Darling Test\n",
    "The Anderson-Darling test is another method for testing whether two samples of data come from the same distribution. A key advantage of the A-D test over the K-S test is its higher sensitivity to differences in the tails of the distributions. This is particularly relevant for financial time series, where capturing tail risk and extreme events is critical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3be0e9-3c73-4051-af0a-a01531953178",
   "metadata": {},
   "outputs": [],
   "source": [
    "KSampleADTest(in_sample_dataset,in_sample_decoded_archive[:,rand(1:number_of_paths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a777e7-1783-4a1e-a785-628c0e81e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    pvalue_cutoff = 0.05; # cutoff\n",
    "    pass_counter = 0;\n",
    "    for i ∈ 1:number_of_paths\n",
    "        test_value = KSampleADTest(in_sample_dataset,in_sample_decoded_archive[:,i]) |> pvalue\n",
    "        if (test_value > pvalue_cutoff)\n",
    "            pass_counter += 1 # we pass (fail to reject) x and y are from the same distribution\n",
    "        end\n",
    "    end\n",
    "    println(\"Pass percentage: $((pass_counter/number_of_paths)*100)%\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8d975-1cd6-45b4-bb7d-93c0ef14b27a",
   "metadata": {},
   "source": [
    "#### Jensen-Shannon Divergence\n",
    "The Jensen-Shannon (J-S) divergence is a method of measuring the similarity between two probability distributions, bounded between 0 (identical distributions) and 1 (maximally different distributions). Unlike the K-S and A-D tests, it is not a hypothesis test but rather a symmetric and smoothed metric of divergence, often used in information theory and for evaluating generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c98a15-8925-4064-97ae-36992f161917",
   "metadata": {},
   "outputs": [],
   "source": [
    "js_divergence(in_sample_dataset,in_sample_decoded_archive[:,rand(1:number_of_paths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae3adef-dd2e-448b-aee4-6c83a7bb6ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2651bf-213b-4715-8482-161ef64c6f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    random_index = rand(1:number_of_paths)\n",
    "    simulated_path = in_sample_decoded_archive[:, random_index]\n",
    "    plot_acf_comparison(Rᵢ, simulated_path, \"ACF of Returns (Non-Jump, Path $(random_index))\", random_index)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71966843-a9ed-4561-b863-6a53e470b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "let\n",
    "    random_index = rand(1:number_of_paths)\n",
    "    simulated_path = in_sample_decoded_archive[:, random_index]\n",
    "    plot_acf_comparison(Rᵢ, simulated_path, \"ACF of |Returns| (Non-Jump, Path $(random_index))\", random_index, is_absolute=true)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd23445-1519-46e4-ba3f-6e80feaca6cc",
   "metadata": {},
   "source": [
    "### Conclusion from the Non-Jump Model Analysis\n",
    "\n",
    "The autocorrelation plots reveal a key insight:\n",
    "* **Success:** The standard HMM successfully replicates the lack of significant autocorrelation in raw returns, consistent with the efficient market hypothesis.\n",
    "* **Failure:** The model completely fails to reproduce the strong, persistent autocorrelation in **absolute returns**. This means the simple HMM **cannot capture volatility clustering**, a critical stylized fact of financial markets.\n",
    "\n",
    "This failure motivates the need for a more sophisticated model, which we explore in `Simulation-Notebook-HMM-WithJumps`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c17389a-3c86-45fb-a649-d6611f6720ef",
   "metadata": {
    "id": "2c17389a-3c86-45fb-a649-d6611f6720ef"
   },
   "source": [
    "## Disclaimer and Risks\n",
    "__This content is offered solely for training and informational purposes__. No offer or solicitation to buy or sell securities or derivative products or any investment or trading advice or strategy is made, given, or endorsed by the teaching team.\n",
    "\n",
    "__Trading involves risk__. Carefully review your financial situation before investing in securities, futures contracts, options, or commodity interests. Past performance, whether actual or indicated by historical tests of strategies, is no guarantee of future performance or success. Trading is generally inappropriate for someone with limited resources, investment or trading experience, or a low-risk tolerance.  Only risk capital that is not required for living expenses.\n",
    "\n",
    "__You are fully responsible for any investment or trading decisions you make__. You should decide solely based on your financial circumstances, investment or trading objectives, risk tolerance, and liquidity needs."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Julia 1.12.0",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b7b3a3-7ef9-47a0-9a48-17617733b7c1",
   "metadata": {},
   "source": [
    "###   __1__: Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ac8cdd-b4a3-4e3c-9d89-5702f9f57980",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"include.jl\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f2b42-82f5-4894-8726-57e83430c7e1",
   "metadata": {},
   "source": [
    "###  __2__: Load the JLD2 file\n",
    "\n",
    "We gathered a daily open-high-low-close `dataset` for each firm in the [S&P500](https://en.wikipedia.org/wiki/S%26P_500) from `01-03-2014` until `02-07-2025`, along with data for a few exchange-traded funds and volatility products during that time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9dfcf54a-02b4-4c42-b94e-c2f259049586",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = MyOriginalPortfolioDataSet();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a1dd50-0651-4100-8933-2f540589fef1",
   "metadata": {},
   "source": [
    "###  __3__: Extract the DataFrame from the loaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c46ec8a8-dc35-4843-afe5-c03344af637d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, DataFrame} with 515 entries:\n",
       "  \"TPR\"  => \u001b[1m1828×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"EMR\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"CTAS\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"HSIC\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"KIM\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"PLD\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"IEX\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"KSU\"  => \u001b[1m2001×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"BAC\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"CBOE\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"EXR\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"NCLH\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"CVS\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"DRI\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"DTE\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"ZION\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"AVY\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"EW\"   => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"EA\"   => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"NWSA\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"BBWI\" => \u001b[1m884×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"CAG\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"GPC\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"FCX\"  => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  \"GILD\" => \u001b[1m2792×8 DataFrame\u001b[0m\u001b[0m…\n",
       "  ⋮      => ⋮"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset = data[\"dataset\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba81c712-25cd-414c-b2e5-0ab09419268a",
   "metadata": {},
   "source": [
    "###  __4__: Extract tickers with the maximum number of trading days as `AAPL`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfeba4-d93a-4a1c-81a0-f2b0f0ba7359",
   "metadata": {
    "id": "4c4f3ad7-5b45-4c75-b247-3741fbed8960"
   },
   "source": [
    "Not all tickers in our dataset have the maximum number of trading days for various reasons, e.g., acquisition or de-listing events. Let's collect only those tickers with the maximum number of traditional days. First, let's compute the number of records for a company that we know has a maximum value, e.g., `AAPL,` and save that value in the `maximum_number_trading_days` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0075f75a-9f20-41ed-800e-010ce721570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2792"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maximum_number_trading_days = original_dataset[\"AAPL\"] |> nrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5175246f-bfdd-46ae-bf6c-7bc55a957f9e",
   "metadata": {
    "id": "7c3f4ad1-6a0e-46ce-935a-cea5f2d54fe7"
   },
   "source": [
    "Then, iterate through our data and collect only tickers with `maximum_number_trading_days` records. This will make it easier to perform analysis on the full dataset such as computing the daily growth rates. Save that data in the `dataset::Dict{String,DataFrame}` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79f5940-834e-4498-8f31-15c121950647",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dict{String,DataFrame}();\n",
    "for (ticker,data) ∈ original_dataset\n",
    "    if (nrow(data) == maximum_number_trading_days)\n",
    "        dataset[ticker] = data;\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c73d25-c62f-4975-9be7-65e04ed4de19",
   "metadata": {
    "id": "b2c9ab47-5fce-4422-81db-26fe609732ec"
   },
   "source": [
    "Lastly, let's get a sorted list of firms that we have in cleaned up `dataset` and save it in the `list_of_all_tickers::Array{String,1}` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0266a66-bc37-44ea-9714-89e610b11aaf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "424-element Vector{String}:\n",
       " \"A\"\n",
       " \"AAL\"\n",
       " \"AAP\"\n",
       " \"AAPL\"\n",
       " \"ABBV\"\n",
       " \"ABT\"\n",
       " \"ACN\"\n",
       " \"ADBE\"\n",
       " \"ADI\"\n",
       " \"ADM\"\n",
       " \"ADP\"\n",
       " \"ADSK\"\n",
       " \"AEE\"\n",
       " ⋮\n",
       " \"WST\"\n",
       " \"WU\"\n",
       " \"WY\"\n",
       " \"WYNN\"\n",
       " \"XEL\"\n",
       " \"XOM\"\n",
       " \"XRAY\"\n",
       " \"XYL\"\n",
       " \"YUM\"\n",
       " \"ZBRA\"\n",
       " \"ZION\"\n",
       " \"ZTS\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_all_tickers = keys(dataset) |> collect |> x->sort(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12da5c-faf8-4b2c-b300-bc4f60e72286",
   "metadata": {},
   "source": [
    "### __5__: Split and save each ticker's data into two dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b331ba-9264-47ff-9e98-33d96dffd67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start = DateTime(2014, 1, 1)\n",
    "train_end = DateTime(2023, 12, 31, 23, 59, 59)\n",
    "\n",
    "train_dataset = Dict{String, DataFrame}()\n",
    "test_dataset = Dict{String, DataFrame}()\n",
    "\n",
    "for ticker in list_of_all_tickers\n",
    "    df = dataset[ticker]\n",
    "    df.timestamp = DateTime.(df.timestamp)  # ensure the timestamp column is DateTime\n",
    "\n",
    "    train_data = filter(row -> train_start <= row.timestamp <= train_end, df)\n",
    "    test_data = filter(row -> row.timestamp > train_end, df)\n",
    "\n",
    "    train_dataset[ticker] = train_data\n",
    "    test_dataset[ticker] = test_data\n",
    "end\n",
    "\n",
    "train_file = joinpath(_PATH_TO_DATA, \"train_dataset_2014_2023.jld2\")\n",
    "test_file = joinpath(_PATH_TO_DATA, \"test_dataset_2024_onward.jld2\")\n",
    "\n",
    "@save train_file train_dataset\n",
    "@save test_file test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a539be1-6bc5-45cd-9800-d662b05eab98",
   "metadata": {},
   "source": [
    "### Disclaimer: For Educational and Research Purposes Only\n",
    "\n",
    "The content in this repository is provided strictly for informational, educational, and research purposes. It is not intended as, and should not be construed as, financial advice, an offer, or a solicitation to buy or sell any securities or derivative products.\n",
    "\n",
    "#### Risk Warning\n",
    "\n",
    "Trading and investing involve substantial risk. The models and strategies demonstrated here are for illustrative purposes only. Past performance, whether actual or backtested, is not a guarantee of future results.\n",
    "\n",
    "You are solely responsible for any investment or trading decisions you make. Always conduct your own research and carefully assess your financial situation, investment objectives, and risk tolerance before trading or investing. You should only risk capital that you can afford to lose and that is not essential for your living expenses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12.1",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
